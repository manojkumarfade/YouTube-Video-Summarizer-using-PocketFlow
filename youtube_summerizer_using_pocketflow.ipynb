{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 1: Install Dependencies\n",
        "\n",
        "- Installs **pocketflow** â†’ lightweight workflow engine (Node + Flow concept)\n",
        "- Installs **yt-dlp** â†’ reliable YouTube subtitle extractor (works in Colab)\n",
        "- Installs **google-generativeai** â†’ Gemini API for summarization\n",
        "- `!pip install` ensures all packages are available in the Colab runtime\n"
      ],
      "metadata": {
        "id": "xYwn0obXTjxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pocketflow yt-dlp google-generativeai\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Vt_qcxanMZXi",
        "outputId": "9dced09b-038d-42fd-fa69-71ac79cf4161"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pocketflow in /usr/local/lib/python3.12/dist-packages (0.0.3)\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.12.8)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 2: Imports and API Configuration\n",
        "\n",
        "- Imports core Python utilities:\n",
        "  - `os` â†’ environment variables & file handling\n",
        "  - `re` â†’ regex for extracting video ID\n",
        "  - `time` â†’ retry wait logic\n",
        "  - `copy` â†’ safe node cloning in PocketFlow\n",
        "  - `subprocess` â†’ execute yt-dlp shell command\n",
        "- Imports `google.generativeai` to access Gemini models\n",
        "- Reads Gemini API key securely from **Colab userdata**\n",
        "- Stores model name in a constant for reuse\n"
      ],
      "metadata": {
        "id": "JwlxBEwjTmw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, time, copy, subprocess\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"RAGAGENTKEY\")\n",
        "MODEL = \"gemini-2.5-flash\"\n"
      ],
      "metadata": {
        "id": "_ylaoyn_QWxp"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 3: BaseNode Class (PocketFlow Core)\n",
        "\n",
        "- Represents a **single step** in a workflow\n",
        "- `successors` stores transitions â†’ action â†’ next node\n",
        "- `next()` links nodes together using action labels\n",
        "- `prep()` â†’ prepares input from shared state\n",
        "- `exec()` â†’ main logic of the node\n",
        "- `post()` â†’ writes output back to shared state\n",
        "- `_run()` enforces execution order: prep â†’ exec â†’ post\n",
        "- `_exec()` allows override (used for retries in child class)\n"
      ],
      "metadata": {
        "id": "IoacRBTYTp0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseNode:\n",
        "    def __init__(self):\n",
        "        self.successors = {}\n",
        "\n",
        "    def next(self, action, node):\n",
        "        self.successors[action] = node\n",
        "        return node\n",
        "\n",
        "    def prep(self, shared): pass\n",
        "    def exec(self, data): pass\n",
        "    def post(self, shared, data, out): return out\n",
        "\n",
        "    def _run(self, shared):\n",
        "        data = self.prep(shared)\n",
        "        out = self._exec(data)\n",
        "        return self.post(shared, data, out)\n",
        "\n",
        "    def _exec(self, data):\n",
        "        return self.exec(data)\n"
      ],
      "metadata": {
        "id": "n6Zt7azQQZLP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 4: Node Class with Retry Support\n",
        "\n",
        "- Extends `BaseNode`\n",
        "- Adds:\n",
        "  - `retries` â†’ number of attempts\n",
        "  - `wait` â†’ delay between retries\n",
        "- `_exec()` wraps `exec()` inside a retry loop\n",
        "- Automatically retries on failure\n",
        "- Useful for unstable operations like:\n",
        "  - network calls\n",
        "  - YouTube caption extraction\n"
      ],
      "metadata": {
        "id": "hE1L30eiTsmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(BaseNode):\n",
        "    def __init__(self, retries=1, wait=0):\n",
        "        super().__init__()\n",
        "        self.retries, self.wait = retries, wait\n",
        "\n",
        "    def _exec(self, data):\n",
        "        for i in range(self.retries):\n",
        "            try:\n",
        "                return self.exec(data)\n",
        "            except:\n",
        "                if i == self.retries - 1:\n",
        "                    raise\n",
        "                time.sleep(self.wait)\n"
      ],
      "metadata": {
        "id": "nvjHDuYOSxsw"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 5: Flow Class\n",
        "\n",
        "- Controls execution of connected nodes\n",
        "- Takes a **starting node**\n",
        "- Executes nodes sequentially based on returned actions\n",
        "- Uses `copy.copy()` to avoid mutating original nodes\n",
        "- Continues until no next node is found\n",
        "- Shared dictionary flows through all nodes\n"
      ],
      "metadata": {
        "id": "0PZZz5V2TuWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flow:\n",
        "    def __init__(self, start):\n",
        "        self.start = start\n",
        "\n",
        "    def run(self, shared):\n",
        "        node = copy.copy(self.start)\n",
        "        while node:\n",
        "            action = node._run(shared)\n",
        "            node = copy.copy(node.successors.get(action))\n"
      ],
      "metadata": {
        "id": "McIBpeU0Sy4Z"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 6: GetVideoID Node\n",
        "\n",
        "- Extracts YouTube video ID from URL\n",
        "- `prep()` reads `video_url` from shared state\n",
        "- `exec()` uses regex to extract the `v=` parameter\n",
        "- `post()` stores `video_id` back into shared state\n",
        "- Returns `\"ok\"` to trigger next node\n"
      ],
      "metadata": {
        "id": "om2pi2d6Tvyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetVideoID(Node):\n",
        "    def prep(self, shared):\n",
        "        return shared[\"video_url\"]\n",
        "\n",
        "    def exec(self, url):\n",
        "        return re.search(r\"v=([^&]+)\", url).group(1)\n",
        "\n",
        "    def post(self, shared, _, vid):\n",
        "        shared[\"video_id\"] = vid\n",
        "        return \"ok\"\n"
      ],
      "metadata": {
        "id": "KACnqN4OQZIq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 7: GetTranscript Node (Using yt-dlp)\n",
        "\n",
        "- Fetches **auto-generated subtitles** from YouTube\n",
        "- Uses yt-dlp via `subprocess`\n",
        "- Flags used:\n",
        "  - `--skip-download` â†’ no video download\n",
        "  - `--write-auto-sub` â†’ auto captions\n",
        "  - `--sub-lang en` â†’ English subtitles\n",
        "  - `--sub-format vtt` â†’ readable subtitle format\n",
        "- Reads `.vtt` file and:\n",
        "  - removes timestamps\n",
        "  - removes metadata\n",
        "  - merges subtitle text\n",
        "- Deletes subtitle file after processing\n",
        "- Saves transcript into shared state\n"
      ],
      "metadata": {
        "id": "o9s52m2CTx4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetTranscript(Node):\n",
        "    def prep(self, shared):\n",
        "        return shared[\"video_id\"]\n",
        "\n",
        "    def exec(self, vid):\n",
        "        cmd = [\n",
        "            \"yt-dlp\",\n",
        "            \"--skip-download\",\n",
        "            \"--write-auto-sub\",\n",
        "            \"--sub-lang\", \"en\",\n",
        "            \"--sub-format\", \"vtt\",\n",
        "            f\"https://www.youtube.com/watch?v={vid}\"\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "\n",
        "        vtt = next(f for f in os.listdir() if f.endswith(\".vtt\"))\n",
        "        with open(vtt, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = \" \".join(\n",
        "                line.strip() for line in f\n",
        "                if line and not line.startswith((\"WEBVTT\", \"Kind:\", \"Language:\")) and \"-->\" not in line\n",
        "            )\n",
        "        os.remove(vtt)\n",
        "        return text\n",
        "\n",
        "    def post(self, shared, _, text):\n",
        "        shared[\"transcript\"] = text\n",
        "        return \"ok\"\n"
      ],
      "metadata": {
        "id": "-xUR1onUQZGL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 8: Summarize Node (Gemini API)\n",
        "\n",
        "- Reads full transcript from shared state\n",
        "- Loads Gemini model (`gemini-2.5-flash`)\n",
        "- Sends transcript with a clear summarization prompt\n",
        "- Receives AI-generated summary text\n",
        "- Stores summary back into shared state\n",
        "- Returns `\"done\"` to end the flow\n"
      ],
      "metadata": {
        "id": "B1NYDX_sTzn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Summarize(Node):\n",
        "    def prep(self, shared):\n",
        "        return shared[\"transcript\"]\n",
        "\n",
        "    def exec(self, text):\n",
        "        model = genai.GenerativeModel(MODEL)\n",
        "        return model.generate_content(\n",
        "            f\"Summarize clearly:\\n{text}\"\n",
        "        ).text\n",
        "\n",
        "    def post(self, shared, _, summary):\n",
        "        shared[\"summary\"] = summary\n",
        "        return \"done\"\n"
      ],
      "metadata": {
        "id": "D06iv08qRUyn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§  Inbuilt Chatbot Assistant (Q&A on Video)\n",
        "\n",
        "At this stage, the YouTube video has already been:\n",
        "- converted into a transcript\n",
        "- summarized once and stored in shared memory\n",
        "\n",
        "This chatbot does **not regenerate the summary**.\n",
        "\n",
        "Instead, it:\n",
        "- takes the existing **summary** and **transcript**\n",
        "- accepts a user question\n",
        "- uses Gemini to answer questions **only related to this video**\n",
        "\n",
        "Think of this as an interactive assistant sitting *on top* of the summary.\n"
      ],
      "metadata": {
        "id": "OujXw9w6dwYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatAssistant(Node):\n",
        "    def prep(self, shared):\n",
        "        return {\n",
        "            \"question\": shared[\"user_question\"],\n",
        "            \"summary\": shared[\"summary\"],\n",
        "            \"transcript\": shared[\"transcript\"]\n",
        "        }\n",
        "\n",
        "    def exec(self, data):\n",
        "        model = genai.GenerativeModel(MODEL)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an assistant answering questions about a YouTube video.\n",
        "\n",
        "Summary:\n",
        "{data['summary']}\n",
        "\n",
        "Transcript (for reference):\n",
        "{data['transcript'][:6000]}\n",
        "\n",
        "User question:\n",
        "{data['question']}\n",
        "\"\"\"\n",
        "\n",
        "        return model.generate_content(prompt).text\n",
        "\n",
        "    def post(self, shared, _, answer):\n",
        "        shared[\"chat_answer\"] = answer\n",
        "        return \"done\"\n"
      ],
      "metadata": {
        "id": "Lj1CmP6PdgD2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 9: Build PocketFlow Graph\n",
        "\n",
        "- Creates node instances:\n",
        "  - GetVideoID\n",
        "  - GetTranscript (with retries)\n",
        "  - Summarize\n",
        "- Links nodes using action-based transitions\n",
        "- Defines execution order:\n",
        "  Video URL â†’ Video ID â†’ Transcript â†’ Summary\n",
        "- Initializes `Flow` with the starting node\n"
      ],
      "metadata": {
        "id": "Se7nbKPST09s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_node = GetVideoID()\n",
        "tx_node = GetTranscript(retries=3, wait=2)\n",
        "sm_node = Summarize()\n",
        "\n",
        "id_node.next(\"ok\", tx_node)\n",
        "tx_node.next(\"ok\", sm_node)\n",
        "\n",
        "youtube_flow = Flow(id_node)\n"
      ],
      "metadata": {
        "id": "Q0iJkuGLQZDv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Cell 10: Shared State Dictionary\n",
        "\n",
        "- Central data store passed across all nodes\n",
        "- Contains:\n",
        "  - `video_url` â†’ input\n",
        "  - `video_id` â†’ extracted ID\n",
        "  - `transcript` â†’ captions text\n",
        "  - `summary` â†’ final output\n",
        "- Enables clean data flow without global variables\n",
        "- Starts workflow execution using `youtube_flow.run()`\n",
        "- Each node reads and writes to shared state\n",
        "- Flow ends automatically after Summarize node\n",
        "- Final summary is printed from shared state\n"
      ],
      "metadata": {
        "id": "5OLTS1GxT2jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shared = {\n",
        "    \"video_url\": \"https://www.youtube.com/watch?v=dyUojOVBEcE\",\n",
        "    \"video_id\": None,\n",
        "    \"transcript\": None,\n",
        "    \"summary\": None\n",
        "}\n",
        "\n",
        "youtube_flow.run(shared)\n",
        "print(shared[\"summary\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "A38al42XQjDF",
        "outputId": "2d6e16f3-1dd7-45ab-a13e-036a8d0ab311"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video explains how to enhance Large Language Models (LLMs) to answer questions using private or company-specific data, overcoming the limitations of relying solely on general training data.\n",
            "\n",
            "The presenter contrasts two main approaches:\n",
            "\n",
            "1.  **Fine-tuning:** Directly retraining an LLM on custom data. This is highlighted as very costly, computationally expensive, time-consuming, and difficult to update frequently.\n",
            "2.  **Retrieval Augmented Generation (RAG):** Presented as a more efficient and practical solution.\n",
            "\n",
            "**How RAG works:**\n",
            "\n",
            "1.  **Data Ingestion:** Relevant information (e.g., website content from educosis.com) is scraped using a `WebBaseLoader`.\n",
            "2.  **Text Splitting:** The content is broken down into smaller, overlapping \"chunks\" using a `RecursiveCharacterTextSplitter`. Overlapping chunks ensure that context is maintained even if a key piece of information is split between two chunks.\n",
            "3.  **Embedding:** These text chunks are converted into numerical \"embeddings\" (vector representations) using `OpenAIEmbeddings`.\n",
            "4.  **Vector Storage:** The embeddings are stored in a vector database (like ChromaDB).\n",
            "\n",
            "When a user asks a question:\n",
            "\n",
            "1.  The system retrieves the most relevant embedded chunks from the vector database based on the user's query.\n",
            "2.  This retrieved context is then \"augmented\" by being added to the original user query, forming a comprehensive prompt.\n",
            "3.  This enriched prompt is sent to the LLM (e.g., OpenAI's GPT model).\n",
            "4.  The LLM generates an answer based on the provided, relevant context, ensuring accuracy and specificity to the custom data.\n",
            "\n",
            "The demonstration illustrates building this RAG pipeline using the LangChain framework, successfully answering specific questions about course recordings, testimonials, and projects from the scraped website data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’¬ Interactive Chat with the Video\n",
        "\n",
        "This section enables a continuous questionâ€“answer loop.\n",
        "\n",
        "How it works:\n",
        "- The summary and transcript are already stored in `shared`\n",
        "- The user can now ask **multiple questions**\n",
        "- Each question is answered using the same video context\n",
        "- Typing `exit` stops the chat\n",
        "\n",
        "This makes the notebook behave like a mini ChatGPT\n",
        "that only knows about this YouTube video.\n"
      ],
      "metadata": {
        "id": "ZmgupNCDd92Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_node = ChatAssistant()\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nAsk something about the video (type 'exit' to stop): \")\n",
        "    if question.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    shared[\"user_question\"] = question\n",
        "    chat_node._run(shared)\n",
        "\n",
        "    print(\"\\nAnswer:\\n\", shared[\"chat_answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "39cJFii6dms9",
        "outputId": "942e3aa9-eb2e-4942-c6ac-c2b3d3eed447"
      },
      "execution_count": 68,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask something about the video (type 'exit' to stop): what is rag\n",
            "\n",
            "Answer:\n",
            " RAG stands for **Retrieval Augmented Generation**.\n",
            "\n",
            "It is presented in the video as a more efficient and practical solution for enhancing Large Language Models (LLMs) to answer questions using private or company-specific data, overcoming the limitations of relying solely on general training data.\n",
            "\n",
            "Here's how RAG works:\n",
            "\n",
            "1.  **Data Ingestion and Processing:**\n",
            "    *   Relevant information (e.g., website content) is collected.\n",
            "    *   This content is then split into smaller, overlapping \"chunks\" to maintain context.\n",
            "    *   These text chunks are converted into numerical \"embeddings\" (vector representations).\n",
            "    *   The embeddings are stored in a vector database.\n",
            "\n",
            "2.  **Question Answering Process:**\n",
            "    *   When a user asks a question, the system first retrieves the most relevant embedded chunks from the vector database based on the user's query.\n",
            "    *   This retrieved context is then \"augmented\" by being added to the original user query, creating a more comprehensive prompt.\n",
            "    *   This enriched prompt is then sent to an LLM (like OpenAI's GPT model).\n",
            "    *   The LLM generates an answer based on the specific, relevant context provided, ensuring accuracy and specificity to the custom data.\n",
            "\n",
            "Ask something about the video (type 'exit' to stop): exit\n"
          ]
        }
      ]
    }
  ]
}